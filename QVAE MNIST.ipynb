{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from neural import QVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "SHUFFLE = True\n",
    "NUM_WORKERS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_loader = data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, transform=transforms.ToTensor(), download=True),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_set_loader = data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.ToTensor(), download=True),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = QVAE(\n",
    "    in_channels=1,\n",
    "    num_hiddens=32,\n",
    "    num_res_hiddens=0,\n",
    "    num_res_layers=0,\n",
    "    rgb_out=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    net.load_state_dict(torch.load(open('state_dict.pth', 'rb')))\n",
    "    print('State Dict loaded from \\'state_dict.pth\\'')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=10):\n",
    "    print('='*10, end='')\n",
    "    print(' TRAIN', end=' ') \n",
    "    print('='*10, end='\\n\\n')\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        running_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_set_loader, 1):\n",
    "            images, _ = batch\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Zero grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            encoded, quantized, recon_x = net(images)\n",
    "            # Compute Loss\n",
    "            loss_value = net.loss_function(images, recon_x, encoded, quantized)\n",
    "            running_loss += loss_value.item()\n",
    "            # Backward\n",
    "            loss_value.backward()\n",
    "            # Update\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'==> EPOCH[{epoch}]({i}/{len(train_set_loader)}): LOSS: {loss_value.item()}')\n",
    "            \n",
    "        print(f'=====> EPOCH[{epoch}] Completed: Avg. LOSS: {running_loss/len(train_set_loader)}')\n",
    "        print()\n",
    "        \n",
    "    net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== TRAIN ==========\n",
      "\n",
      "==> EPOCH[1](100/469): LOSS: 0.09839844703674316\n",
      "==> EPOCH[1](200/469): LOSS: 0.0843266025185585\n",
      "==> EPOCH[1](300/469): LOSS: 0.07667625695466995\n",
      "==> EPOCH[1](400/469): LOSS: 0.07573133707046509\n",
      "=====> EPOCH[1] Completed: Avg. LOSS: 0.10011284699889897\n",
      "\n",
      "==> EPOCH[2](100/469): LOSS: 0.07628166675567627\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-638e03af54d7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Compute Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/qvae-rACEtkm2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/qvae/neural/qvae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m         )\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mquantized_skip_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquantized\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/qvae-rACEtkm2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/qvae/neural/qvae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m# Calculate distances (Euclidean distance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flatten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/qvae-rACEtkm2/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.quantizer.num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QVAE(\n",
       "  (encoder): ResEncoder(\n",
       "    (conv_1): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv_3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (res_stack): ResStack(\n",
       "      (res_layers): ModuleList()\n",
       "    )\n",
       "  )\n",
       "  (pre_qunatization_conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (quantizer): Quantizer(\n",
       "    (embeddings): Embedding(1, 8)\n",
       "  )\n",
       "  (decoder): ResDecoder(\n",
       "    (conv_1): Conv2d(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (res_stack): ResStack(\n",
       "      (res_layers): ModuleList()\n",
       "    )\n",
       "    (conv_trans_1): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv_trans_2): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAL8ElEQVR4nO3dXagc9R3G8eepGhD1IjZLPJxIY9UbLTTKEiqKpISKLxdRBDGCpCBEIYJCLhRF1LtQ60svihBrSFqsIr5ghJBqQ0QCIq4hiVFptRIxh5hsCGokqI359eKMcoxn5xx3Zne2+X0/cNjZ+e/uPCx5Mrszu/t3RAjAie9nTQcAMByUHUiCsgNJUHYgCcoOJHHyMDc2b968WLhw4TA3CaSyZ88eHTx40NONVSq77Ssl/UnSSZL+EhFrym6/cOFCdTqdKpsEUKLdbvcc6/tlvO2TJP1Z0lWSLpC03PYF/T4egMGq8p59saQPI+KjiPhG0jOSltUTC0DdqpR9XNInU67vLdb9gO2Vtju2O91ut8LmAFQx8KPxEbE2ItoR0W61WoPeHIAeqpR9QtLZU64vKNYBGEFVyv6WpPNtn2N7jqQbJW2sJxaAuvV96i0ijtq+XdI/NHnqbV1EvFtbMgC1qnSePSI2SdpUUxYAA8THZYEkKDuQBGUHkqDsQBKUHUiCsgNJUHYgCcoOJEHZgSQoO5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kQdmBJCg7kARlB5Kg7EASlB1IYqhTNuPE8/XXX5eOX3/99T3H7GlnFv7eyy+/3FcmTI89O5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kwXl2VPLGG2+Ujr/22ms9x7Zu3VpzGpSpVHbbeyQdlvStpKMR0a4jFID61bFn/21EHKzhcQAMEO/ZgSSqlj0kvWL7bdsrp7uB7ZW2O7Y73W634uYA9Ktq2S+LiIslXSVple3Lj79BRKyNiHZEtFutVsXNAehXpbJHxERxeUDSi5IW1xEKQP36Lrvt02yf8d2ypCsk7a4rGIB6VTkaP1/Si8V3kk+W9PeI2FxLKoyMiCgdn+k8+5EjR3qOHT16tK9M6E/fZY+IjyT9usYsAAaIU29AEpQdSIKyA0lQdiAJyg4kwVdcUWrnzp2l4/fee2/p+FlnndVz7JJLLukrE/rDnh1IgrIDSVB2IAnKDiRB2YEkKDuQBGUHkuA8O0qtX7++6QioCXt2IAnKDiRB2YEkKDuQBGUHkqDsQBKUHUiC8+zJzfRzztu3b6/0+EuXLq10f9SHPTuQBGUHkqDsQBKUHUiCsgNJUHYgCcoOJMF59uQ+++yz0vFt27ZVevwHH3yw0v1Rnxn37LbX2T5ge/eUdWfaftX2B8Xl3MHGBFDVbF7Gr5d05XHr7pa0JSLOl7SluA5ghM1Y9oh4XdKh41Yvk7ShWN4g6dqacwGoWb8H6OZHxL5i+VNJ83vd0PZK2x3bnW632+fmAFRV+Wh8RISkKBlfGxHtiGi3Wq2qmwPQp37Lvt/2mCQVlwfqiwRgEPot+0ZJK4rlFZJeqicOgEGZ8Ty77aclLZE0z/ZeSfdLWiPpWdu3SPpY0g2DDInBee655yrd/7zzzisdX7BgQaXHR31mLHtELO8xxK8SAP9H+LgskARlB5Kg7EASlB1IgrIDSfAV1+QeeuihSve/5pprSsfnzJlT6fFRH/bsQBKUHUiCsgNJUHYgCcoOJEHZgSQoO5AE59mTO3LkSNMRMCTs2YEkKDuQBGUHkqDsQBKUHUiCsgNJUHYgCc6zn+BmmpL5q6++GlISNI09O5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kwXn2E1yn0ykd//zzzys9/qpVqyrdH8Mz457d9jrbB2zvnrLuAdsTtncUf1cPNiaAqmbzMn69pCunWf9oRCwq/jbVGwtA3WYse0S8LunQELIAGKAqB+hut72reJk/t9eNbK+03bHd6Xa7FTYHoIp+y/64pHMlLZK0T9LDvW4YEWsjoh0R7Var1efmAFTVV9kjYn9EfBsRxyQ9IWlxvbEA1K2vstsem3L1Okm7e90WwGiY8Ty77aclLZE0z/ZeSfdLWmJ7kaSQtEfSrQPMiAoee+yxSve/8MILS8fHx8crPT6GZ8ayR8TyaVY/OYAsAAaIj8sCSVB2IAnKDiRB2YEkKDuQBF9xRamxsbHS8VNPPXVISVAVe3YgCcoOJEHZgSQoO5AEZQeSoOxAEpQdSILz7CeAiYmJnmObN2+u9Nh33XVXpftjdLBnB5Kg7EASlB1IgrIDSVB2IAnKDiRB2YEkOM9+Ajh27FhfY5IUEaXjJ5/MP5ETBXt2IAnKDiRB2YEkKDuQBGUHkqDsQBKUHUiCk6jJ2W46AoZkxj277bNtb7X9nu13bd9RrD/T9qu2Pygu5w4+LoB+zeZl/FFJqyPiAkm/kbTK9gWS7pa0JSLOl7SluA5gRM1Y9ojYFxHbi+XDkt6XNC5pmaQNxc02SLp2UCEBVPeTDtDZXijpIklvSpofEfuKoU8lze9xn5W2O7Y73W63QlQAVcy67LZPl/S8pDsj4oupYzH5bYppv1EREWsjoh0R7VarVSksgP7Nquy2T9Fk0Z+KiBeK1fttjxXjY5IODCYigDrM5mi8JT0p6f2IeGTK0EZJK4rlFZJeqj8egLrM5jz7pZJulvSO7R3FunskrZH0rO1bJH0s6YbBRARQhxnLHhHbJPX65MXSeuMAGBQ+LgskQdmBJCg7kARlB5Kg7EASfMUVpe67777S8dtuu610/KabbqozDipgzw4kQdmBJCg7kARlB5Kg7EASlB1IgrIDSXCe/QQwNjbWc2z16tWl93344YdLx8fHx0vHlyxZUjqO0cGeHUiCsgNJUHYgCcoOJEHZgSQoO5AEZQeS8ORkLsPRbrej0+kMbXtANu12W51OZ9pfg2bPDiRB2YEkKDuQBGUHkqDsQBKUHUiCsgNJzGZ+9rNtb7X9nu13bd9RrH/A9oTtHcXf1YOPC6Bfs/nxiqOSVkfEdttnSHrb9qvF2KMR8cfBxQNQl9nMz75P0r5i+bDt9yWV/3wJgJHzk96z214o6SJJbxarbre9y/Y623N73Gel7Y7tTrfbrRQWQP9mXXbbp0t6XtKdEfGFpMclnStpkSb3/NP+mFlErI2IdkS0W61WDZEB9GNWZbd9iiaL/lREvCBJEbE/Ir6NiGOSnpC0eHAxAVQ1m6PxlvSkpPcj4pEp66f+pOl1knbXHw9AXWZzNP5SSTdLesf2jmLdPZKW214kKSTtkXTrQBICqMVsjsZvkzTd92M31R8HwKDwCTogCcoOJEHZgSQoO5AEZQeSoOxAEpQdSIKyA0lQdiAJyg4kQdmBJCg7kARlB5Kg7EASQ52y2XZX0sdTVs2TdHBoAX6aUc02qrkksvWrzmy/iIhpf/9tqGX/0cbtTkS0GwtQYlSzjWouiWz9GlY2XsYDSVB2IImmy7624e2XGdVso5pLIlu/hpKt0ffsAIan6T07gCGh7EASjZTd9pW2/2X7Q9t3N5GhF9t7bL9TTEPdaTjLOtsHbO+esu5M26/a/qC4nHaOvYayjcQ03iXTjDf63DU9/fnQ37PbPknSvyX9TtJeSW9JWh4R7w01SA+290hqR0TjH8CwfbmkLyX9NSJ+Vaz7g6RDEbGm+I9ybkTcNSLZHpD0ZdPTeBezFY1NnWZc0rWSfq8Gn7uSXDdoCM9bE3v2xZI+jIiPIuIbSc9IWtZAjpEXEa9LOnTc6mWSNhTLGzT5j2XoemQbCRGxLyK2F8uHJX03zXijz11JrqFoouzjkj6Zcn2vRmu+95D0iu23ba9sOsw05kfEvmL5U0nzmwwzjRmn8R6m46YZH5nnrp/pz6viAN2PXRYRF0u6StKq4uXqSIrJ92CjdO50VtN4D8s004x/r8nnrt/pz6tqouwTks6ecn1BsW4kRMREcXlA0osavamo9383g25xeaDhPN8bpWm8p5tmXCPw3DU5/XkTZX9L0vm2z7E9R9KNkjY2kONHbJ9WHDiR7dMkXaHRm4p6o6QVxfIKSS81mOUHRmUa717TjKvh567x6c8jYuh/kq7W5BH5/0i6t4kMPXL9UtLO4u/dprNJelqTL+v+q8ljG7dI+rmkLZI+kPRPSWeOULa/SXpH0i5NFmusoWyXafIl+i5JO4q/q5t+7kpyDeV54+OyQBIcoAOSoOxAEpQdSIKyA0lQdiAJyg4kQdmBJP4HczKtQ7O3oHIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = test_set_loader.dataset[430]\n",
    "encoded, quantized, recon = net(image.unsqueeze(0))\n",
    "\n",
    "print(label)\n",
    "plt.imshow(image[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "recon = recon[0].squeeze()\n",
    "plt.imshow(recon.detach().numpy(), cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), open('state_dict.pth', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
